{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd7dbc-3b61-4845-8b7f-d7114bbd63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess a dataset, we need to follow the following steps:\n",
    "\n",
    "\n",
    "Handling missing values: We can handle missing values by either dropping the rows or columns with missing values or by imputing the missing values. Imputation can be done by replacing the missing values with the mean, median, mode, or any other value that makes sense for the data.\n",
    "Encoding categorical variables: Categorical variables need to be encoded into numerical values before they can be used in a machine learning model. This can be done using techniques such as one-hot encoding, label encoding, or binary encoding.\n",
    "Scaling numerical features: Scaling is necessary when the numerical features have different scales or units. This can be done using techniques such as standardization or normalization.\n",
    "\n",
    "Here is an example code snippet in Python using scikit-learn library to preprocess a dataset:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['Age'] = imputer.fit_transform(data[['Age']])\n",
    "\n",
    "# Encoding categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "cat_features = ['Gender', 'City']\n",
    "encoded = encoder.fit_transform(data[cat_features])\n",
    "data = pd.concat([data.drop(cat_features, axis=1), pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(cat_features))], axis=1)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = StandardScaler()\n",
    "num_features = ['Age', 'Income']\n",
    "data[num_features] = scaler.fit_transform(data[num_features])\n",
    "\n",
    "In this example, we first load the dataset from a CSV file. Then we handle missing values in the 'Age' column by imputing the mean value. Next, we encode the categorical variables 'Gender' and 'City' using one-hot encoding. Finally, we scale the numerical features 'Age' and 'Income' using standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7477a-6ed8-441e-9581-3697f1bf63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "To split the dataset into a training set and a test set, we can use the train_test_split function from the scikit-learn library in Python. Here is an example code snippet:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data.drop('Target', axis=1), data['Target'], test_size=0.3, random_state=42)\n",
    "\n",
    "In this example, we first specify the features and target variable of the dataset. Then we use the train_test_split function to split the data into a training set and a test set. The test_size parameter is set to 0.3, which means that 30% of the data will be used for testing and 70% will be used for training. The random_state parameter is set to 42 to ensure that the same split is obtained every time the code is run. The resulting variables train_data, test_data, train_labels, and test_labels contain the training features, testing features, training target variable, and testing target variable respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0869074-97a5-48c3-bfa1-f3e2e379d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "To evaluate the performance of the trained random forest classifier on the test set, we can use scikit-learn's classification_report function, which computes and prints the precision, recall, F1 score, and support for each class in the target variable. Here's an example code snippet:\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the target variable for the test set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Compute and print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "In the above code, X_test and y_test are the input features and target variable of the test set respectively. The predict method of the trained random forest classifier is used to predict the target variable for the test set. Finally, the classification_report function is used to compute and print the precision, recall, F1 score, and support for each class in the target variable.\n",
    "\n",
    "\n",
    "Note that in order to compute these metrics, we need to have a ground truth label for each data point in the test set. Therefore, we need to have a separate set of labeled data that was not used during training or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1aedc5-fdb8-4d40-ad12-50910e610e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the top 5 most important features in predicting heart disease risk, we can use the feature_importances_ attribute of the trained random forest classifier. Here's an example code snippet:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importances from the trained random forest classifier\n",
    "importances = rfc.feature_importances_\n",
    "\n",
    "# Get the indices of the top 5 most important features\n",
    "indices = importances.argsort()[-5:]\n",
    "\n",
    "# Get the names of the top 5 most important features\n",
    "features = X.columns[indices]\n",
    "\n",
    "# Plot a bar chart of the feature importances\n",
    "plt.bar(features, importances[indices])\n",
    "plt.title(\"Top 5 Most Important Features\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "In the above code, X is the pandas DataFrame containing the input features used during training. The feature_importances_ attribute of the trained random forest classifier is used to get the importance scores for each feature. The argsort method is used to get the indices of the top 5 most important features, and then these indices are used to get the names of these features from the X.columns attribute. Finally, a bar chart is plotted using Matplotlib to visualize the feature importances.\n",
    "\n",
    "\n",
    "Note that this is just an example code snippet and you may need to modify it based on your specific dataset and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bb363-0755-41c1-8b56-a460b2feda75",
   "metadata": {},
   "outputs": [],
   "source": [
    "To tune the hyperparameters of the random forest classifier using grid search or random search, we can use scikit-learn's GridSearchCV or RandomizedSearchCV classes. Here's an example code snippet using GridSearchCV:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a grid search object with the random forest classifier and hyperparameter grid\n",
    "grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "In the above code, X_train and y_train are the input features and target variable of the training set respectively. The param_grid dictionary defines the hyperparameters to search over and their possible values. The GridSearchCV class is used to create a grid search object with the random forest classifier and hyperparameter grid. The cv parameter specifies the number of folds for cross-validation. Finally, the fit method of the grid search object is used to fit the object to the training data and find the best set of hyperparameters.\n",
    "\n",
    "\n",
    "Note that this is just an example code snippet and you may need to modify it based on your specific dataset and use case. You can also use RandomizedSearchCV instead of GridSearchCV if you want to search over a random subset of the hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc676a-8357-4cd0-95c4-f686774e5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "After running the grid search or random search, we can report the best set of hyperparameters found by the search and the corresponding performance metrics using the best_params_ and best_score_ attributes of the search object. Here's an example code snippet:\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the performance of the tuned model on the test set\n",
    "y_pred_tuned = grid_search.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "# Print the performance metrics of the tuned model\n",
    "print(\"Tuned Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_tuned)\n",
    "print(\"Precision:\", precision_tuned)\n",
    "print(\"Recall:\", recall_tuned)\n",
    "print(\"F1 Score:\", f1_tuned)\n",
    "\n",
    "# Evaluate the performance of the default model on the test set\n",
    "y_pred_default = rfc.predict(X_test)\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "precision_default = precision_score(y_test, y_pred_default)\n",
    "recall_default = recall_score(y_test, y_pred_default)\n",
    "f1_default = f1_score(y_test, y_pred_default)\n",
    "\n",
    "# Print the performance metrics of the default model\n",
    "print(\"Default Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_default)\n",
    "print(\"Precision:\", precision_default)\n",
    "print(\"Recall:\", recall_default)\n",
    "print(\"F1 Score:\", f1_default)\n",
    "\n",
    "In the above code, X_test and y_test are the input features and target variable of the test set respectively. The accuracy_score, precision_score, recall_score, and f1_score functions from scikit-learn are used to compute the performance metrics of the tuned and default models on the test set. Finally, the performance metrics of the tuned and default models are printed for comparison.\n",
    "\n",
    "\n",
    "Note that this is just an example code snippet and you may need to modify it based on your specific dataset and use case. The best set of hyperparameters found by the search and the corresponding performance metrics will depend on your specific dataset and search parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
